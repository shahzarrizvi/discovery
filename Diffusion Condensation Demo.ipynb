{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph Coarsening #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from condensation import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project explores a method for graph coarsening utilizing an [inhomogeneous diffusion condensation](https://arxiv.org/abs/1907.04463) algorithm which has already shown success in the coarse graining of data.\n",
    "\n",
    "The goal of graph coarsening is to take a very complex graph—one with many nodes and many connections—and \"condense\" the graph by combining similar nodes into one another. The result of graph coarsening is a coarsened graph consisting of supernodes with edges between them, where the supernodes are linear combinations of the nodes in the original graph, and the edges maintain the connections from the original graph.\n",
    "\n",
    "At the same time, we would like to maintain a kernel throughout the coarsening of this graph. In general, a kernel is a function which specifies the similarities between two datapoints; this \"similarity function\" acts as an inner product over the set of datapoints. For machine learning purposes, defining a kernel on a graph is desirable. One such graph kernel is called the marginalized graph kernel and finds the similarity between two graphs by comparing the similarities of random walks on both graphs. Such a graph kernel requires a nodel kernel and an edge kernel (kernel functions computing the similarities between nodes and edges). Since the original nodes of the graph are condensed over the course of the graph coarsening, a node kernel must be defined at each step of the process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inhomogeneous Diffusion Condensation #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can first explore the process of inhomogeneous diffusion condensation. \n",
    "\n",
    "This is a clustering algorithm similar to $k$-means or agglomerative clustering which seeks to find a hierarchical coarse-grained representation of the data. The algorithm works by computing a diffusion operator $\\mathbf{P}$ (consisting of the transition matrix of a Markovian random walk over the data) and repeatedly applying it to the data.\n",
    "\n",
    "The details of the algorithm are unimportant, but it is interesting to apply it to various datasets. Here, we have eight datasets (``barbell``, ``tree``, ``noisy tree``, ``clusters``, ``uniform circle``, ``hyperuniform circle``, ``hyperuniform ellipse``, and ``two spirals``) on which we can apply the diffusion condensation algorithm. These datasets come from this [github repository](https://github.com/matthew-hirn/condensation) on inhomogeneous diffusion condensation.\n",
    "\n",
    "The datasets are visualized below for ``N`` points. You can change ``N`` and run the cell to see the datasets with a different amount of points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 2 ** 7\n",
    "\n",
    "plot_datasets(N)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
